{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'klaam'\n",
      "/Users/mohamed/Documents/NBE Project/customer-speech-profiling/klaam/klaam\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'klaam' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Install Dependencies (Comment after installation)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# %%capture\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# %pip install -r requirements.txt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m get_ipython()\u001b[39m.\u001b[39mrun_line_magic(\u001b[39m'\u001b[39m\u001b[39mcd\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mklaam\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m klaam\u001b[39m/\u001b[39mklaam\u001b[39m/\u001b[39minstall\n",
      "\u001b[0;31mNameError\u001b[0m: name 'klaam' is not defined"
     ]
    }
   ],
   "source": [
    "# Install Dependencies (Comment after installation)\n",
    "# %%capture\n",
    "# %pip install -r requirements.txt\n",
    "%cd klaam\n",
    "# klaam/klaam/install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Huggingface libraries and pipeline\n",
    "\n",
    "import os\n",
    "import wave\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio, display\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import sounddevice as sd\n",
    "\n",
    "print(\"Loading libraries...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre: Record audio from user or load audio from file\n",
    "### 1. Record audio from user for 10 seconds\n",
    "### 2. Load audio from file (wav file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Options:\n",
      "1. Record audio\n",
      "2. Browse and play audio\n",
      "3. Exit\n",
      "File not found.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wave\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sounddevice as sd\n",
    "\n",
    "audio = None\n",
    "\n",
    "def record_audio(device_id, duration, sample_rate):\n",
    "    global audio\n",
    "    print(\"Recording audio...\")\n",
    "    audio_data = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1, device=device_id)\n",
    "    sd.wait()\n",
    "    print(\"Recording complete.\")\n",
    "    audio = audio_data\n",
    "\n",
    "def save_wav_file(filename, audio_data, sample_rate):\n",
    "    with wave.open(filename, 'wb') as wf:\n",
    "        wf.setnchannels(1)\n",
    "        wf.setsampwidth(2)\n",
    "        wf.setframerate(sample_rate)\n",
    "        wf.writeframes(audio_data.tobytes())\n",
    "\n",
    "def plot_audio_waveform(audio_data, sample_rate):\n",
    "    time = np.arange(0, len(audio_data) / sample_rate, 1 / sample_rate)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(time, audio_data)\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.title(\"Audio Waveform\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "def browse_audio_file(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        with wave.open(file_path, 'rb') as wf:\n",
    "            audio_data = wf.readframes(-1)\n",
    "            audio_data = np.frombuffer(audio_data, dtype=np.int16)\n",
    "            return audio_data\n",
    "    else:\n",
    "        print(\"File not found.\")\n",
    "        return None\n",
    "\n",
    "# Parameters\n",
    "duration = 5  # Recording duration in seconds\n",
    "sample_rate = 44100  # Audio sample rate\n",
    "\n",
    "# Command-line interface\n",
    "while True:\n",
    "    print(\"Options:\")\n",
    "    print(\"1. Record audio\")\n",
    "    print(\"2. Browse and play audio\")\n",
    "    print(\"3. Exit\")\n",
    "    choice = input(\"Enter your choice (1/2/3): \")\n",
    "\n",
    "    if choice == '1':\n",
    "        device_id = int(input(\"Enter device ID: \"))\n",
    "        record_audio(device_id, duration, sample_rate)\n",
    "        save_wav_file(\"recorded_audio.wav\", audio, sample_rate)\n",
    "        print(\"Recording saved as 'recorded_audio.wav'\")\n",
    "        plot_audio_waveform(audio, sample_rate)\n",
    "\n",
    "        break\n",
    "\n",
    "    elif choice == '2':\n",
    "        file_path = input(\"Enter the path to the audio file (wav): \")\n",
    "        audio_data = browse_audio_file(file_path)\n",
    "        if audio_data is not None:\n",
    "            plot_audio_waveform(audio_data, sample_rate)\n",
    "        \n",
    "        break\n",
    "\n",
    "    elif choice == '3':\n",
    "        break\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid choice. Please choose again.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise reduction on recorded audio\n",
    "import noisereduce as nr\n",
    "from scipy.io import wavfile\n",
    "rate, data = wavfile.read('recorded_audio.wav')\n",
    "reduced_noise = nr.reduce_noise(y=data, sr=rate)\n",
    "wavfile.write(\"reduced_noise.wav\", rate, reduced_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Arabic Speech to text [Without post processing]\n",
    "Converts .wav files to text using Arabic Speech to text model\n",
    "Needs a language model to improve the results and correct the spelling mistakes\n",
    "\n",
    "### Model Used:\n",
    "- [Arabic Speech to text](https://huggingface.co/arbml/wav2vec2-large-xlsr-53-arabic-egyptian)\n",
    "\n",
    "### Dataset Used:\n",
    "?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Load model from huggingface\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# from transformers import AutoProcessor, AutoModelForCTC\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[39m# Load klaam model\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mklaam\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mklaam\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrun\u001b[39;00m \u001b[39mimport\u001b[39;00m SpeechRecognition\n\u001b[1;32m      9\u001b[0m model \u001b[39m=\u001b[39m SpeechRecognition()\n\u001b[1;32m     10\u001b[0m model\u001b[39m.\u001b[39mtranscribe(wav_file)\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "# Load model from huggingface\n",
    "# from transformers import AutoProcessor, AutoModelForCTC\n",
    "\n",
    "# processor = AutoProcessor.from_pretrained(\"arbml/wav2vec2-large-xlsr-53-arabic-egyptian\")\n",
    "# model = AutoModelForCTC.from_pretrained(\"arbml/wav2vec2-large-xlsr-53-arabic-egyptian\")\n",
    "\n",
    "# Load klaam model\n",
    "from .klaam.klaam.run import SpeechRecognition\n",
    "model = SpeechRecognition()\n",
    "model.transcribe(wav_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "def transcribe_with_wav2vec(audio):\n",
    "    input_values = processor(audio, return_tensors=\"pt\", sampling_rate=16_000).input_values\n",
    "    resampler = torchaudio.transforms.Resample(48_000, 16_000)\n",
    "    input_values = resampler(input_values)\n",
    "    logits = model(input_values).logits\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    transcription = processor.decode(predicted_ids[0])\n",
    "    return transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Correcting the output of the model using contextual processing and spell checking\n",
    "Using chatgpt to generate the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Initialize the OpenAI API client\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "# Transcribe audio using WAV2VEC (replace with your WAV2VEC code)\n",
    "asr_transcription = transcribe_with_wav2vec(audio)\n",
    "\n",
    "# Generate refined transcription using GPT-3\n",
    "prompt = f\"Audio transcription: {asr_transcription}\\nRefine the transcription:\"\n",
    "language_model_response = openai.Completion.create(\n",
    "    engine=\"davinci\",\n",
    "    prompt=prompt,\n",
    "    max_tokens=100,\n",
    "    n=1  # Number of responses\n",
    ")\n",
    "refined_transcription = language_model_response.choices[0].text.strip()\n",
    "\n",
    "print(\"Original ASR Transcription:\", asr_transcription)\n",
    "print(\"Refined Transcription:\", refined_transcription)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
